---
layout: post
title: Gradient Descent Algorithm
date: 2025-03-22
description: You’ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. # Add post description (optional)
img: post-cvx-1.png # Add image post (optional)
tags: [Blog, Convex Optimization]
author: # Add name author (optional)
---

# 本讲内容

梯度下降算法 (Gradient Descent Algorithm): 通过迭代找到目标函数的收敛最小值。梯度下降法的基本思想可以类比为一个下山的过程。假设这样一个场景：一个人被困在山上，需要从山上下来(找到山的最低点，也就是山谷)。但此时山上的浓雾很大，导致可视度很低；因此，下山的路径就无法确定，必须利用自己周围的信息一步一步地找到下山的路。这个时候，便可利用梯度下降算法来帮助自己下山。怎么做呢，首先以他当前的所处的位置为基准，寻找这个位置最陡峭的地方，然后朝着下降方向走一步，然后又继续以当前位置为基准，再找最陡峭的地方，再走直到最后到达最低处；同理上山也是如此，只是这时候就变成梯度上升算法了。

---

## 梯度下降

有一个可微分的函数, 找到这个函数的最小值: 因为梯度的方向就是函数变化最快的方向, 所以要找到给定点的梯度; 反复迭代求得梯度，最后就能到达局部的最小值。

微分可以看作函数图像中某点的切线的斜率, 或者函数的变化率; 梯度实际上就是多变量微分的一般化。在单变量的函数中，梯度是函数的微分，代表着函数在某个给定点的切线的斜率; 在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向, 所以梯度的反方向就是函数在给定点下降最快的方向。

<figure style="text-align: center;">
<img src="/assets/img/cvx1.png" alt="梯度公式" width="300">
<figcaption></figcaption>
</figure>

## 梯度下降与机器学习

在机器学习算法中，在最小化损失函数时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数，和模型参数值。反过来，如果我们需要求解损失函数的最大值，这时就需要用梯度上升法来迭代了。

梯度下降法和梯度上升法是可以互相转化的。比如我们需要求解损失函数f(θ)的最小值，这时我们需要用梯度下降法来迭代求解。但是实际上，我们可以反过来求解损失函数 -f(θ)的最大值，这时梯度上升法就派上用场了。

梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。

步长（Learning rate）: 步长决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度。用上面下山的例子，步长就是在当前这一步所在位置沿着最陡峭最易下山的位置走的那一步的长度。

特征（feature）：指的是样本中输入部分，比如2个单特征的样本 (x<sup>(0)</sup>, y<sup>(0)</sup>), (x<sup>(1)</sup>, y<sup>(1)</sup>),则第一个样本特征为 x<sup>(0)</sup>, 第一个样本输出为 y<sup>(0)</sup>.

假设函数（hypothesis function）：在监督学习中，为了拟合输入样本，而使用的假设函数，记为 h<sub>θ</sub>(x). 比如对于单个特征的m个样本 (x<sup>(i)</sup>, y<sup>(i)</sup>), (i=1,2,...m), 可以采用拟合函数如下：h<sub>θ</sub>(x) = θ<sub>0</sub> + θ<sub>1</sub>x.

损失函数（loss function）：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。在线性回归中，损失函数通常为样本输出和假设函数的差取平方。比如对于m个样本 (x<sup>(i)</sup>, y<sup>(i)</sup>), (i=1,2,...m),, 采用线性回归，损失函数为：

## 梯度下降的表示

梯度下降法的算法可以有代数法和矩阵法（也称向量法）两种表示，如果对矩阵分析不熟悉，则代数法更加容易理解。不过矩阵法更加的简洁，且由于使用了矩阵，实现逻辑更加的一目了然。这里先介绍代数法，后介绍矩阵法。

### 代数方式

**先决条件**: 确认优化模型的假设函数和损失函数。

比如对于线性回归，假设函数表示为 

<figure style="text-align: center;">
<img src="/assets/img/cvx1_1.png" alt="" width="300">
<figcaption></figcaption>
</figure>

其中 θ<sub>i</sub>(i = 0,1,...,n) 为模型参数; x<sub>i</sub>(i = 0,1,...,n) 为每个样本的n个特征值。这个表示可以简化，我们增加一个特征 x<sub>0</sub> = 1, 这样就得到:

<figure style="text-align: center;">
<img src="/assets/img/cvx1_2.png" alt="" width="300">
<figcaption></figcaption>
</figure>

同样是线性回归，对应于上面的假设函数，损失函数为：

<figure style="text-align: center;">
<img src="/assets/img/cvx1_3.png" alt="" width="300">
<figcaption></figcaption>
</figure>

**算法相关参数初始化**：主要是初始化 θ<sub>i</sub>,算法终止距离
ε以及步长α。

**算法过程**：
1. 确定当前位置的损失函数的梯度，对于 θ<sub>i</sub> ,其梯度表达式如下：

<figure style="text-align: center;">
<img src="/assets/img/cvx1_4.png" alt="" width="300">
<figcaption></figcaption>
</figure>

2. 用步长乘以损失函数的梯度，得到当前位置下降的距离，即

<figure style="text-align: center;">
<img src="/assets/img/cvx1_5.png" alt="" width="300">
<figcaption></figcaption>
</figure>

3. 确定是否所有的 θ<sub>i</sub>,梯度下降的距离都小于ε，如果小于ε则算法终止，当前所有的 θ<sub>i</sub>(i=0,1,...n)即为最终结果。否则进入步骤4.
4. 更新所有的θ，对于 θ<sub>i</sub>，其更新表达式如下。更新完毕后继续转入步骤1.

<figure style="text-align: center;">
<img src="/assets/img/cvx1_6.png" alt="" width="300">
<figcaption></figcaption>
</figure>






### 矩阵方式