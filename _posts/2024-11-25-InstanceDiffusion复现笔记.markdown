---
layout: post
title:  InstanceDiffuison 复现笔记
date:   2024-11-25
description: You’ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. # Add post description (optional)
img: instdiff.png # Add image post (optional)
tags: [Blog]
author: # Add name author (optional)
---
# 本讲内容

其实这篇是在某个不知名的睡不着的下午(2025-8-9)写的, 想回忆一下去年尝试这篇的过程记录一下, 否则花了很多时间什么也没学到感觉很可惜. 记录的时间选择的是文件夹最后更新的时间.

先放一个文章链接: [Instance Diffusion][instdiff]

[instdiff]: https://arxiv.org/pdf/2402.03290.pdf

当时刚来这边上学, 到一个新环境加上觉得学校水平很高, 所以干什么都摩拳擦掌跃跃欲试. 选了一门期末要求复现顶刊项目的课, 课倒是好说, 但是项目不自量力地选了一篇看上去很高大上很强的(没有说人家实际上不好的意思), 觉得只要自己愿意花时间啃下来就肯定能复现出来, 加上对这个题目挺感兴趣的, 就定了这篇论文. 当然可能因为自己还是实力不够吧, 最终没能成功复现, 当然设备限制也是一个很大的原因, 在运行关键步骤报了一个致命性错误, 抱着两台没有gpu无用武之地的mac手足无措, 最终也没找到解决方法. 当时很长一段时间跑不出来特别焦虑, 甚至想退课重选一门, 当然最后答辩是基于我已有的结果做了报告, 助教蛮好的, 老师也没多挑毛病, 还是以自我感觉比较好的成绩过了. 难道真诚真的是必杀技吗(哈哈). 

今天想起来这个项目已经是去年冬天做的了, 答辩一结束我就把文件夹拖到U盘删了, 它这个数据集太巨大了, 最小的包都要5GB. ppt倒是还有一点总结的内容不知道能不能找回来, 我试着找一找吧.(事实证明好像找不回来了, 原来课程网站也关闭了, 写的word倒是从circuit上下回来了) 自结课之后也有好多次想做个总结记下来, 但是总有一种不敢回头看的感觉, 也说不上原因. 也许是觉得回头看一下就是对自己的能力再多质疑一次, 还是挺伤心的. 而且这个项目有点连锁反应的感觉, 我要会这个项目, 就必须会它用到的某一个模型A, 要使用模型A, 我就得部署一下B, B必须用CUDA...像这样buff叠buff, 就失败了. 哎, 反正今天想起来了, 先开一个文档, 不行就每天写一点呗, 反正都过去咯.


## 文生图

InstanceDiffusion（Wang 等人，2024）的目标是允许在图像生成过程中进行实例级操作，从而克服扩散模型的局限性。扩散模型虽然能够生成高质量图像，但无法轻松控制或编辑图像中的单个对象。这一进步对于视觉设计、广告和创意内容生成等领域至关重要，因为这些领域用户通常需要专注于特定元素。能够精确控制单个元素、处理冲突信息以及模拟对象之间复杂的交互变得越来越重要。本文通过实现对图像中单个实例的精确控制，解决了当前生成模型的一个关键限制。在人工智能/机器学习领域，扩散模型在生成高质量图像方面展现出了令人印象深刻的能力，但它们通常缺乏控制生成场景中特定对象或元素的能力。这种控制力的缺乏限制了它们在需要定制化和精细可控性的应用中的实用性，在这些应用中，用户可能需要在不影响整个构图的情况下操作图像的特定部分。

## 相关工作

### Diffusion Models



### Open-Set Object Detection(OSOD)


### Controlable Generation






## Instance Diffusion

大家都知道stable diffusion. 本文提出了 InstanceDiffusion 模型。它可以控制图像生成中的“实例级”元素，这意味着该系统可以处理图像中的不同对象，专注于生成能够精确控制场景中单个实例的图像。与专注于整幅图像的传统模型相比，该模型可以更精细地控制每个对象在图像中的呈现和定位方式。它允许用户为每个实例提供文本描述并指定位置，其中位置描述可以是边界框、蒙版、点或涂鸦。就是说能很细节地控制生成对象的属性.

它引入了一种允许实例级控制的新方法，这意味着可以编辑、替换或操作图像中的单个对象或区域，而不会影响其他部分。这一进步弥合了通用生成模型与特定领域应用之间的差距——特定领域应用需要高度定制，例如设计广告中的特定元素或改进创意艺术作品的各个方面。在人工智能与实际应用的交叉领域，实例级控制是一个强大的工具，可以显著增强人工智能在许多现实世界应用中的应用，从而实现更有针对性和更实用的成果。此外，提高生成模型可控性所涉及的技术挑战，也为我们加深对高级机器学习技术的理解提供了一个激动人心的机会。

总的来说，这项工作为上述单一控制图像生成任务提供了一个强大的模型工具。
在实施任务中，我们专注于如何训练模型，即深度学习网络的架构，
而不会讨论如何将模型与现有的AI工具（例如Compy-UI）结合使用。因为模型的使用是本文的成果，而非工作本身。

### 数据集

数据集准备主要包括三个步骤：图像级标签生成；边界框和掩码生成；实例级文本提示生成。
多模态语言视觉模型通常基于数亿个图文对进行训练，例如 CLIP 和 DALL-E。对于大多数研究人员来说，收集这样的数据集仍然相当困难。作者建议使用 LAION-400M 数据集来获取满足要求的原始数据。这个庞大的资源包含 4 亿对图像 URL 及其对应的元数据；4 亿对 CLIP 图像嵌入及其对应的文本；以及 img2dataset 库，该库能够以最少的资源从 URL 列表中高效地抓取和处理数亿张图像及其元数据。但基于我自己的实现，值得注意的是，在实际操作中，计算机可能无法快速处理如此多的各种格式的文件。下图给出了一个示例。

<figure style="text-align: center;">
<img src="/assets/img/inst1.png" alt="" width="500">
<figcaption>原始数据结构展示</figcaption>
</figure>

放一个数据集[官网][laion]和下载[链接][download]:

[laion]: https://laion.ai/blog/laion-400-open-dataset/
[download]: https://www.kaggle.com/datasets/romainbeaumont/laion400m 或 https://arxiv.org/abs/2111.02114

我们应该有一个包含图片路径和图片标题的“train data.json”文件。经过一些处理后，将获得所需的json文件（名为output\output.json），其中包含每张图片的文本描述和文件地址，并与以下内容对齐。

<figure style="text-align: center;">
<img src="/assets/img/inst2.png" alt="" width="400">
<figcaption>数据生成</figcaption>
</figure>

