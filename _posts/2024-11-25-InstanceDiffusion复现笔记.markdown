---
layout: post
title:  InstanceDiffuison 复现笔记
date:   2024-11-25
description: You’ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. # Add post description (optional)
img: instdiff.png # Add image post (optional)
tags: [Blog]
author: # Add name author (optional)
---
# 本讲内容

其实这篇是在某个不知名的睡不着的下午(2025-8-9)写的, 想回忆一下去年尝试这篇的过程记录一下, 否则花了很多时间什么也没学到感觉很可惜. 记录的时间选择的是文件夹最后更新的时间.

先放一个文章链接: [Instance Diffusion][instdiff]

[instdiff]: https://arxiv.org/pdf/2402.03290.pdf

当时刚来这边上学, 到一个新环境加上觉得学校水平很高, 所以干什么都摩拳擦掌跃跃欲试. 选了一门期末要求复现顶刊项目的课, 课倒是好说, 但是项目不自量力地选了一篇看上去很高大上很强的(没有说人家实际上不好的意思), 觉得只要自己愿意花时间啃下来就肯定能复现出来, 加上对这个题目挺感兴趣的, 就定了这篇论文. 当然可能因为自己还是实力不够吧, 最终没能成功复现, 当然设备限制也是一个很大的原因, 在运行关键步骤报了一个致命性错误, 抱着两台没有gpu无用武之地的mac手足无措, 最终也没找到解决方法. 当时很长一段时间跑不出来特别焦虑, 甚至想退课重选一门, 当然最后答辩是基于我已有的结果做了报告, 助教蛮好的, 老师也没多挑毛病, 还是以自我感觉比较好的成绩过了. 难道真诚真的是必杀技吗(哈哈). 

今天想起来这个项目已经是去年冬天做的了, 答辩一结束我就把文件夹拖到U盘删了, 它这个数据集太巨大了, 最小的包都要5GB. ppt倒是还有一点总结的内容不知道能不能找回来, 我试着找一找吧.(事实证明好像找不回来了, 原来课程网站也关闭了, 写的word倒是从circuit上下回来了) 自结课之后也有好多次想做个总结记下来, 但是总有一种不敢回头看的感觉, 也说不上原因. 也许是觉得回头看一下就是对自己的能力再多质疑一次, 还是挺伤心的. 而且这个项目有点连锁反应的感觉, 我要会这个项目, 就必须会它用到的某一个模型A, 要使用模型A, 我就得部署一下B, B必须用CUDA...像这样buff叠buff, 就失败了. 哎, 反正今天想起来了, 先开一个文档, 不行就每天写一点呗, 反正都过去咯.


## 文生图

InstanceDiffusion（Wang 等人，2024）的目标是允许在图像生成过程中进行实例级操作，从而克服扩散模型的局限性。扩散模型虽然能够生成高质量图像，但无法轻松控制或编辑图像中的单个对象。这一进步对于视觉设计、广告和创意内容生成等领域至关重要，因为这些领域用户通常需要专注于特定元素。能够精确控制单个元素、处理冲突信息以及模拟对象之间复杂的交互变得越来越重要。本文通过实现对图像中单个实例的精确控制，解决了当前生成模型的一个关键限制。在人工智能/机器学习领域，扩散模型在生成高质量图像方面展现出了令人印象深刻的能力，但它们通常缺乏控制生成场景中特定对象或元素的能力。这种控制力的缺乏限制了它们在需要定制化和精细可控性的应用中的实用性，在这些应用中，用户可能需要在不影响整个构图的情况下操作图像的特定部分。

## 相关工作

### Diffusion Models



### Open-Set Object Detection(OSOD)


### Controlable Generation






## Instance Diffusion

大家都知道stable diffusion. 本文提出了 InstanceDiffusion 模型。它可以控制图像生成中的“实例级”元素，这意味着该系统可以处理图像中的不同对象，专注于生成能够精确控制场景中单个实例的图像。与专注于整幅图像的传统模型相比，该模型可以更精细地控制每个对象在图像中的呈现和定位方式。它允许用户为每个实例提供文本描述并指定位置，其中位置描述可以是边界框、蒙版、点或涂鸦。就是说能很细节地控制生成对象的属性.

它引入了一种允许实例级控制的新方法，这意味着可以编辑、替换或操作图像中的单个对象或区域，而不会影响其他部分。这一进步弥合了通用生成模型与特定领域应用之间的差距——特定领域应用需要高度定制，例如设计广告中的特定元素或改进创意艺术作品的各个方面。在人工智能与实际应用的交叉领域，实例级控制是一个强大的工具，可以显著增强人工智能在许多现实世界应用中的应用，从而实现更有针对性和更实用的成果。此外，提高生成模型可控性所涉及的技术挑战，也为我们加深对高级机器学习技术的理解提供了一个激动人心的机会。

总的来说，这项工作为上述单一控制图像生成任务提供了一个强大的模型工具。
在实施任务中，我们专注于如何训练模型，即深度学习网络的架构，
而不会讨论如何将模型与现有的AI工具（例如Compy-UI）结合使用。因为模型的使用是本文的成果，而非工作本身。

### 数据集

数据集准备主要包括三个步骤：图像级标签生成；边界框和掩码生成；实例级文本提示生成。
多模态语言视觉模型通常基于数亿个图文对进行训练，例如 CLIP 和 DALL-E。对于大多数研究人员来说，收集这样的数据集仍然相当困难。作者建议使用 LAION-400M 数据集来获取满足要求的原始数据。这个庞大的资源包含 4 亿对图像 URL 及其对应的元数据；4 亿对 CLIP 图像嵌入及其对应的文本；以及 img2dataset 库，该库能够以最少的资源从 URL 列表中高效地抓取和处理数亿张图像及其元数据。但基于我自己的实现，值得注意的是，在实际操作中，计算机可能无法快速处理如此多的各种格式的文件。下图给出了一个示例。

<figure style="text-align: center;">
<img src="/assets/img/inst1.png" alt="" width="500">
<figcaption>原始数据结构展示</figcaption>
</figure>

放一个数据集[官网][laion]和下载[链接][download]:

[laion]: https://laion.ai/blog/laion-400-open-dataset/
[download]: https://www.kaggle.com/datasets/romainbeaumont/laion400m 或 https://arxiv.org/abs/2111.02114

我们应该有一个包含图片路径和图片标题的“train data.json”文件。经过一些处理后，将获得所需的json文件（名为output\output.json），其中包含每张图片的文本描述和文件地址，并与以下内容对齐。

<figure style="text-align: center;">
<img src="/assets/img/inst2.png" alt="" width="400">
<figcaption>数据生成</figcaption>
</figure>

### 模型结构

只写两个关键创新模块.

**UniFusion Block:**
UniFusion 类的核心功能是将不同类型的“实例信息”融合到生成模型的输入中。这里的“实例信息”指的是图像中某些特定对象或区域的描述信息（例如框、点、涂鸦、蒙版等）。
利用这些信息，我们可以控制图像生成模型生成的细节，并让模型“知道”需要关注哪些特定对象或区域。
通过使用傅里叶嵌入、条件卷积和可选的独立线性处理器，UniFusion 模块可以灵活地嵌入各种实例条件（例如框、点和蒙版）。在训练过程中，模型会随机“丢弃”一些输入以增强鲁棒性；而在测试过程中，模型可以根据条件选择是否使用这些输入。这种设计提高了实例控制的准确性，并允许多种输入组合，为细粒度图像生成提供了更强大的控制能力。


<figure style="text-align: center;">
<img src="/assets/img/inst3.png" alt="" width="400">
<figcaption>UniFusion</figcaption>
</figure>

此代码定义了用于实例控制图像生成任务的 InstanceDiffusion 模型的一些超参数：将涂鸦的点数设置为
20，以表示涂鸦的形状和位置；将
多边形（即蒙版边缘）的点数设置为
256，以细化蒙版形状；将傅里叶嵌入的频率设置为对边界、点等的位置坐标进行傅里叶嵌入，以增强位置编码；布尔值决定模型是否在训练期间添加框、点、涂鸦和蒙版作为额外的控制输入，以生成针对特定实例的条件控制；当
use-separate-tokenizer 设置为 True 时，每种输入类型将使用单独的编码器或线性层进行处理，以保留其自身特性，帮助模型更好地融合和区分不同类型的控制信号。初始化超参数，以便在后续处理中能够根据不同类型的输入控制信号调整生成过程。

**ScaleU block:**
基于 UNet（Ronneberger、Fischer 和 Brox，2015），
对 UNet 的主要特征和跳跃特征进行处理和加权。主要特征和跳跃特征都设计了一个参数来控制其权重。
ScaleULayer 是 UNetModel 中一个特定函数的一部分，用于实现一种名为“ScaleU”的特征调整机制。在 UNetModel 中，它使用 ScaleU 来控制输出分支（即输出块）中某些层的特征图的缩放和调整。在 UNetModel 的初始化方法中，将 enable scaleu 设置为 True，表示启用 ScaleU 机制。
此外，该模型在输出分支中注册了多个缩放参数（scaleu b 和 scaleu s），用于控制特征的缩放和过滤。在前向函数的输出阶段，UNet 通过检查 enable scaleu 是否已启用来决定是否在特定层上应用 ScaleU 机制。每个输出块在前向传播过程中会通过 scaleu b 和 scaleu s 调整其特征图。scaleu b 用于缩放每个特征的幅度，从而控制特征的强度；而 scaleu s 用于通过傅里叶滤波操作调整特征的频率和层级。这种调整方法使网络能够更精细地控制不同特征尺度下的特征图。

<figure style="text-align: center;">
<img src="/assets/inst4.png" alt="" width="400">
<img src="/assets/inst5.png" alt="" width="400">
<figcaption>ScaleU</figcaption>
</figure>

### 实验过程

